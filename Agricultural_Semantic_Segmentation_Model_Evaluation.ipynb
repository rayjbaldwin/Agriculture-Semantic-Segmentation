{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQMYI1KyhJcc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f173c75b-3ff5-4c59-de60-656b6a636a5c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/106.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m106.7/106.7 kB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/58.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.8/58.8 kB\u001B[0m \u001B[31m5.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m12.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "!pip install -q torchsummary\n",
    "!pip install -q segmentation-models-pytorch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kINjxYp3mdD4",
    "outputId": "8cfe483c-1e0a-48d0-8579-f9893d848194"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/V4A/final_val_multi2\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "dataset_path = os.path.join('/content/drive/My Drive', 'V4A/final_val_multi2')\n",
    "\n",
    "print(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHxf1pwEm1Sv"
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = '/content/drive/My Drive/V4A/final_val_multi2/rgb/'\n",
    "MASK_PATH = '/content/drive/My Drive/V4A/final_val_multi2/Combined_Masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwSY99TEnXEu",
    "outputId": "8144516a-8430-4e69-d00c-f1d447ab4013"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Images:  750\n"
     ]
    }
   ],
   "source": [
    "n_classes = 4\n",
    "\n",
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split('.')[0])\n",
    "\n",
    "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n",
    "\n",
    "df = create_df()\n",
    "print('Total Images: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BP0r1XPAnXMl",
    "outputId": "45b2332a-d32d-45b8-b84c-7019d8743583"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Size   :  0\n",
      "Test Size    :  750\n"
     ]
    }
   ],
   "source": [
    "X_train = []  # Empty list for training set since you're not using it\n",
    "X_test = df['id'].values  # Assigning all samples to the test set\n",
    "\n",
    "print('Train Size   : ', len(X_train))\n",
    "print('Test Size    : ', len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF0cx67hzkAp"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_path, mask_path, X,transform=None, patch=False):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        self.patches = patch\n",
    "        # self.mean = mean\n",
    "        # self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            aug = self.transform(image=img, mask=mask)\n",
    "            img = Image.fromarray(aug['image'])\n",
    "            mask = aug['mask']\n",
    "\n",
    "        if self.transform is None:\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "        t = T.Compose([T.ToTensor()])\n",
    "        img = t(img)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        if self.patches:\n",
    "            img, mask = self.tiles(img, mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    # def tiles(self, img, mask):\n",
    "\n",
    "    #     img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768)\n",
    "    #     img_patches  = img_patches.contiguous().view(3,-1, 512, 768)\n",
    "    #     img_patches = img_patches.permute(1,0,2,3)\n",
    "\n",
    "    #     mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n",
    "    #     mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n",
    "\n",
    "    #     return img_patches, mask_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTbqaHrL0JqQ"
   },
   "outputs": [],
   "source": [
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "\n",
    "t_test = None  # No transformations for testing\n",
    "\n",
    "# Test dataset\n",
    "test_set = CustomDataset(IMAGE_PATH, MASK_PATH, X_test,transform=t_test, patch=False)\n",
    "\n",
    "# Test data loader\n",
    "batch_size = 16\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)  # No need to shuffle for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D89Nx1YP5eQG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dc276ccd-119f-44cc-bba0-f707ee2abd65"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m106.7/106.7 kB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.8/58.8 kB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# !pip install -q segmentation-models-pytorch\n",
    "# !pip install -q torchsummary\n",
    "\n",
    "# from torchsummary import summary\n",
    "# import segmentation_models_pytorch as smp\n",
    "# # Define the path to the saved model\n",
    "# model_path = '/content/RJB_514_Unet-Mobilenetv2-transformations.pth'\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = torch.load(model_path, map_location=device)\n",
    "\n",
    "# # If the model was saved with DataParallel, you may need to unwrap it\n",
    "# if isinstance(model, torch.nn.DataParallel):\n",
    "#     model = model.module\n",
    "\n",
    "# # Make sure to move the model to the appropriate device (CPU or GPU) if necessary\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NG9EQZxI0dp_"
   },
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMYj2Fwb0nDj"
   },
   "outputs": [],
   "source": [
    "# def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
    "#     with torch.no_grad():\n",
    "#         pred_mask = F.softmax(pred_mask, dim=1)\n",
    "#         pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "#         pred_mask = pred_mask.contiguous().view(-1)\n",
    "#         mask = mask.contiguous().view(-1)\n",
    "\n",
    "#         iou_per_class = []\n",
    "#         for clas in range(0, n_classes): #loop per pixel class\n",
    "#             true_class = pred_mask == clas\n",
    "#             true_label = mask == clas\n",
    "\n",
    "#             if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "#                 iou_per_class.append(np.nan)\n",
    "#             else:\n",
    "#                 intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "#                 union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "#                 iou = (intersect + smooth) / (union +smooth)\n",
    "#                 iou_per_class.append(iou)\n",
    "#         return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzVGz_PcgfbH"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union + smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        mean_iou = np.nanmean(iou_per_class)\n",
    "        return mean_iou, iou_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haa7RA_Db4Dk"
   },
   "outputs": [],
   "source": [
    "def fit(epochs, model, test_loader, criterion, optimizer, scheduler, patch=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    test_losses = []\n",
    "    val_iou = []\n",
    "    val_acc = []\n",
    "    lrs = []\n",
    "    forward_times = []\n",
    "    backward_times = []\n",
    "    val_iou_per_class = [[] for _ in range(n_classes)]\n",
    "    val_acc_per_class = [[] for _ in range(n_classes)]\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Turn off gradient computation for evaluation\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        val_iou_score = 0\n",
    "        overall_correct = 0\n",
    "        overall_total = 0\n",
    "        class_correct = [0] * n_classes\n",
    "        class_total = [0] * n_classes\n",
    "\n",
    "        # Loop over the test dataset\n",
    "        for i, data in enumerate(tqdm(test_loader)):\n",
    "            # Reshape to patches if necessary\n",
    "            image_tiles, mask_tiles = data\n",
    "\n",
    "            if patch:\n",
    "                bs, n_tiles, c, h, w = image_tiles.size()\n",
    "\n",
    "                image_tiles = image_tiles.view(-1, c, h, w)\n",
    "                mask_tiles = mask_tiles.view(-1, h, w)\n",
    "\n",
    "            image = image_tiles.to(device)\n",
    "            mask = mask_tiles.to(device)\n",
    "            output = model(image)\n",
    "\n",
    "            # Evaluation metrics\n",
    "            val_iou_score, val_iou_per_class_batch = mIoU(output, mask)\n",
    "            val_iou.append(val_iou_score)\n",
    "            for j in range(n_classes):\n",
    "                val_iou_per_class[j].append(val_iou_per_class_batch[j])\n",
    "\n",
    "            # Pixel accuracy per class\n",
    "            pred_labels = torch.argmax(output, dim=1)\n",
    "            correct_per_class = pred_labels.eq(mask).float()\n",
    "            for j in range(n_classes):\n",
    "                class_correct[j] += torch.sum(correct_per_class[mask == j])\n",
    "                class_total[j] += torch.sum(mask == j)\n",
    "\n",
    "            # Overall pixel accuracy\n",
    "            overall_correct += torch.sum(pred_labels == mask)\n",
    "            overall_total += mask.numel()\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(output, mask)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    # Calculate pixel accuracy overall\n",
    "    overall_accuracy = overall_correct.item() / overall_total\n",
    "    val_acc.append(overall_accuracy)\n",
    "\n",
    "    # Calculate pixel accuracy per class\n",
    "    for j in range(n_classes):\n",
    "        if class_total[j] == 0:\n",
    "            val_acc_per_class[j].append(0)\n",
    "        else:\n",
    "            val_acc_per_class[j].append(class_correct[j].item() / class_total[j])\n",
    "\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    val_iou.append(val_iou_score / len(test_loader))\n",
    "\n",
    "    history = {'val_loss': test_losses,\n",
    "               'val_miou': val_iou,\n",
    "               'val_acc': val_acc,\n",
    "               'val_miou_per_class': val_iou_per_class,\n",
    "               'val_acc_per_class': val_acc_per_class,\n",
    "               'forward_times': forward_times,\n",
    "               'backward_times': backward_times\n",
    "               }\n",
    "\n",
    "    print('Total time: {:.2f} m'.format((time.time() - fit_time) / 60))\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "535763aaca514cacb8ce2a82edefbb92",
      "a25f7683909c4a7291b46c9289f5d93f",
      "e36e9cf856e2480684b1ed75d3f32685",
      "c3a4cb85e92648c4abcdd33932bfb39b",
      "895b8ca72db947f2a051e06d0760bae1",
      "2809026fc387425fbb7c18c53ed4ff7c",
      "ed67466e8958447e8b2abe1b5f440316",
      "b28618c93ce14b27b9acdf6a6630c326",
      "21c8f913d4f24e59adbbebbcc742b909",
      "628356cbf9f44216a4d7bf4137a5f7a8",
      "e762590e866b4023a480942769689a3d"
     ]
    },
    "id": "VY6AuOu71JqW",
    "outputId": "29325fe4-4425-46d0-a19a-d0fb4f96eb6a"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "535763aaca514cacb8ce2a82edefbb92"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total time: 5.19 m\n"
     ]
    }
   ],
   "source": [
    "# max_lr = 1e-3\n",
    "# epoch = 15\n",
    "# weight_decay = 1e-4\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "# sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n",
    "#                                             steps_per_epoch=len(test_loader))\n",
    "\n",
    "# history = fit(epoch, model, test_loader, criterion, optimizer, sched)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# model_paths = [\"/content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth\", \"/content/RJB_514_Unet-Mobilenetv2-no-resize-no-transform.pth\", \"/content/RJB_514_Unet-Mobilenetv2-transformations.pth\", \"/content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth\"]\n",
    "model_paths = [\"/content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth\",\"/content/RJB_514_Unet-Mobilenetv2-transformations.pth\", \"/content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth\", \"/content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth\"]\n",
    "max_lr = 1e-3\n",
    "epoch = 15\n",
    "weight_decay = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for model_path in model_paths:\n",
    "    # Load the model\n",
    "    model = torch.load(model_path)\n",
    "\n",
    "    # Initialize optimizer and scheduler for the model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch, steps_per_epoch=len(test_loader))\n",
    "\n",
    "    # Fit the model\n",
    "    history = fit(epoch, model, test_loader, criterion, optimizer, sched)\n",
    "\n",
    "    # Get metrics\n",
    "    val_iou_per_class = history['val_miou_per_class']\n",
    "    val_iou = history['val_miou']\n",
    "    val_acc_per_class = history['val_acc_per_class']\n",
    "    val_acc = history['val_acc']\n",
    "\n",
    "    val_mean_iou_per_class = [np.nanmean(iou_class) for iou_class in val_iou_per_class]\n",
    "\n",
    "    for i, mean_iou in enumerate(val_mean_iou_per_class):\n",
    "        print(\"Validation Mean IoU for Class {} for model {}: {:.4f}\".format(i, model_path, mean_iou))\n",
    "\n",
    "    print(\"Mean IoU for test for model {}: {:.4f}\".format(model_path, val_iou[0]))  # Access the first (and only) element of the list\n",
    "    print(\"Overall Pixel Accuracy for model {}: {:.4f}\".format(model_path, val_acc[0]))\n",
    "\n",
    "    for i, acc_class in enumerate(val_acc_per_class):\n",
    "        print(\"Pixel Accuracy for Class {} for model {}: {:.4f}\".format(i, model_path, acc_class[0]))  # Access the first (and only) element of the list\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVDKRTEDfRo6",
    "outputId": "7d2b0d26-c05c-4963-b594-f67a97176a62"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 47/47 [05:16<00:00,  6.74s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total time: 5.28 m\n",
      "Validation Mean IoU for Class 0 for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.6263\n",
      "Validation Mean IoU for Class 1 for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.0334\n",
      "Validation Mean IoU for Class 2 for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.0000\n",
      "Validation Mean IoU for Class 3 for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.0285\n",
      "Mean IoU for test for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.2867\n",
      "Overall Pixel Accuracy for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.6190\n",
      "Pixel Accuracy for Class 0 for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.9492\n",
      "Pixel Accuracy for Class 1 for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.0360\n",
      "Pixel Accuracy for Class 2 for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.0000\n",
      "Pixel Accuracy for Class 3 for model /content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth: 0.0289\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 47/47 [00:19<00:00,  2.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total time: 0.32 m\n",
      "Validation Mean IoU for Class 0 for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.6439\n",
      "Validation Mean IoU for Class 1 for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.0213\n",
      "Validation Mean IoU for Class 2 for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.0000\n",
      "Validation Mean IoU for Class 3 for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.0089\n",
      "Mean IoU for test for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.3253\n",
      "Overall Pixel Accuracy for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.6443\n",
      "Pixel Accuracy for Class 0 for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.9980\n",
      "Pixel Accuracy for Class 1 for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.0080\n",
      "Pixel Accuracy for Class 2 for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.0000\n",
      "Pixel Accuracy for Class 3 for model /content/RJB_514_Unet-Mobilenetv2-transformations.pth: 0.0097\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 47/47 [00:19<00:00,  2.43it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total time: 0.32 m\n",
      "Validation Mean IoU for Class 0 for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.6125\n",
      "Validation Mean IoU for Class 1 for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.1323\n",
      "Validation Mean IoU for Class 2 for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.0000\n",
      "Validation Mean IoU for Class 3 for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.0591\n",
      "Mean IoU for test for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.3065\n",
      "Overall Pixel Accuracy for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.6217\n",
      "Pixel Accuracy for Class 0 for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.9180\n",
      "Pixel Accuracy for Class 1 for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.1580\n",
      "Pixel Accuracy for Class 2 for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.0000\n",
      "Pixel Accuracy for Class 3 for model /content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth: 0.0756\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 47/47 [00:18<00:00,  2.61it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total time: 0.30 m\n",
      "Validation Mean IoU for Class 0 for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.6068\n",
      "Validation Mean IoU for Class 1 for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.1391\n",
      "Validation Mean IoU for Class 2 for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.0000\n",
      "Validation Mean IoU for Class 3 for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.1547\n",
      "Mean IoU for test for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.3216\n",
      "Overall Pixel Accuracy for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.6192\n",
      "Pixel Accuracy for Class 0 for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.8894\n",
      "Pixel Accuracy for Class 1 for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.1605\n",
      "Pixel Accuracy for Class 2 for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.0000\n",
      "Pixel Accuracy for Class 3 for model /content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth: 0.2282\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oGVaqKv1dCW8",
    "outputId": "110ebbeb-b95d-4428-8a79-adf10aa9ce01"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Mean IoU for Class 0: 0.6439\n",
      "Validation Mean IoU for Class 1: 0.0213\n",
      "Validation Mean IoU for Class 2: 0.0000\n",
      "Validation Mean IoU for Class 3: 0.0089\n",
      "Mean IoU for test: 0.3253\n",
      "Overall Pixel Accuracy: 0.6443\n",
      "Pixel Accuracy for Class 0: 0.9980\n",
      "Pixel Accuracy for Class 1: 0.0080\n",
      "Pixel Accuracy for Class 2: 0.0000\n",
      "Pixel Accuracy for Class 3: 0.0097\n"
     ]
    }
   ],
   "source": [
    "val_iou_per_class = history['val_miou_per_class']\n",
    "val_iou = history['val_miou']\n",
    "val_acc_per_class = history['val_acc_per_class']\n",
    "val_acc = history['val_acc']\n",
    "\n",
    "val_mean_iou_per_class = [np.nanmean(iou_class) for iou_class in val_iou_per_class]\n",
    "\n",
    "for i, mean_iou in enumerate(val_mean_iou_per_class):\n",
    "    print(\"Validation Mean IoU for Class {}: {:.4f}\".format(i, mean_iou))\n",
    "\n",
    "print(\"Mean IoU for test: {:.4f}\".format(val_iou[0]))  # Access the first (and only) element of the list\n",
    "print(\"Overall Pixel Accuracy: {:.4f}\".format(val_acc[0]))\n",
    "\n",
    "for i, acc_class in enumerate(val_acc_per_class):\n",
    "    print(\"Pixel Accuracy for Class {}: {:.4f}\".format(i, acc_class[0]))  # Access the first (and only) element of the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz1FsZwsGInP"
   },
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "# def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
    "#     with torch.no_grad():\n",
    "#         pred_mask = F.softmax(pred_mask, dim=1)\n",
    "#         pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "#         pred_mask = pred_mask.contiguous().view(-1)\n",
    "#         mask = mask.contiguous().view(-1)\n",
    "\n",
    "#         iou_per_class = []\n",
    "#         for clas in range(0, n_classes): # loop per pixel class\n",
    "#             true_class = pred_mask == clas\n",
    "#             true_label = mask == clas\n",
    "\n",
    "#             intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "#             union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "#             iou = (intersect + smooth) / (union + smooth)\n",
    "#             iou_per_class.append(iou)\n",
    "\n",
    "#         mean_iou = np.nanmean(iou_per_class)\n",
    "#         return mean_iou, iou_per_class\n",
    "\n",
    "# import random\n",
    "\n",
    "# # Define class colors and titles\n",
    "# class_colors = {\n",
    "#     0: [91, 6, 117],  # Background purple\n",
    "#     1: [255, 0, 0],   # Dry down is red\n",
    "#     2: [0, 0, 255],   # Nutrient deficiency is blue\n",
    "#     3: [0, 153, 0],     # Weed cluster is black\n",
    "#     # Add more colors for additional classes if needed\n",
    "# }\n",
    "\n",
    "# class_titles = {\n",
    "#     0: 'Background',\n",
    "#     1: 'Dry down',\n",
    "#     2: 'Nutrient deficiency',\n",
    "#     3: 'Weed cluster',\n",
    "#     # Add titles for additional classes if needed\n",
    "# }\n",
    "\n",
    "# # Randomly select 10 indices from the test set\n",
    "# selected_indices = list(range(1, 11)) + list(range(50, 61)) + list(range(100, 111)) + list(range(150, 161)) + list(range(200, 211)) + list(range(285, 296))\n",
    "\n",
    "# # Plotting for each randomly selected image\n",
    "# for idx in selected_indices:\n",
    "#     image, mask = test_set[idx]\n",
    "\n",
    "#     # Convert image and mask to PyTorch tensors\n",
    "#     image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "#     mask = mask.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "#     # Forward pass through the model to get the predicted mask\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         output = model(image)  # Forward pass\n",
    "#         pred_mask = torch.argmax(output, dim=1).squeeze().cpu()  # Convert tensor to CPU and remove batch dimension\n",
    "\n",
    "#     # Convert predicted mask to RGB image with class colors\n",
    "#     pred_mask_rgb = torch.zeros((3, pred_mask.shape[0], pred_mask.shape[1]), dtype=torch.uint8)\n",
    "#     for class_idx, color in class_colors.items():\n",
    "#         pred_mask_rgb[:, pred_mask == class_idx] = torch.tensor(color, dtype=torch.uint8).view(3, 1)\n",
    "\n",
    "#     # Calculate Intersection over Union (IoU)\n",
    "#     iou, iou_per_class = mIoU(output, mask)\n",
    "\n",
    "#     # Plotting\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "#     ax1.imshow(image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "#     ax1.set_title('Picture')\n",
    "#     ax1.axis('off')\n",
    "\n",
    "#     ax2.imshow(mask.squeeze().cpu().numpy())\n",
    "#     ax2.set_title('Ground truth')\n",
    "#     ax2.axis('off')\n",
    "\n",
    "#     ax3.imshow(pred_mask_rgb.permute(1, 2, 0).cpu().numpy())\n",
    "#     ax3.set_title(f'UNet-MobileNetv2 (Predicted)\\nIoU: {iou:.4f}')  # Include IoU in the title\n",
    "#     ax3.axis('off')\n",
    "\n",
    "#     # Add legend for class colors with class titles\n",
    "#     handles = [plt.Rectangle((0, 0), 1, 1, color=np.array(color) / 255) for color in class_colors.values()]\n",
    "#     labels = [class_titles[class_idx] for class_idx in class_colors.keys()]\n",
    "#     ax3.legend(handles, labels)\n",
    "#     fig.patch.set_alpha(0)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): # loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "            union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "            iou = (intersect + smooth) / (union + smooth)\n",
    "            iou_per_class.append(iou)\n",
    "\n",
    "        mean_iou = np.nanmean(iou_per_class)\n",
    "        return mean_iou, iou_per_class\n",
    "\n",
    "# Define class colors and titles\n",
    "class_colors = {\n",
    "    0: [91, 6, 117],  # Background purple\n",
    "    1: [255, 0, 0],   # Dry down is red\n",
    "    2: [0, 0, 255],   # Nutrient deficiency is blue\n",
    "    3: [0, 153, 0],   # Weed cluster is green\n",
    "}\n",
    "\n",
    "class_titles = {\n",
    "    0: 'Background',\n",
    "    1: 'Dry down',\n",
    "    2: 'Nutrient deficiency',\n",
    "    3: 'Weed cluster',\n",
    "}\n",
    "\n",
    "# Load the models\n",
    "model_paths = [\"/content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth\",\"/content/RJB_514_Unet-Mobilenetv2-transformations.pth\", \"/content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth\", \"/content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth\"]\n",
    "models = []\n",
    "for path in model_paths:\n",
    "    model = torch.load(path)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "# Randomly select indices from the test set\n",
    "selected_indices = list(range(1, 55)) + list(range(150, 200)) + list(range(250, 300)) + list(range(450, 500)) + list(range(545, 600)) + list(range(700, 725))\n",
    "\n",
    "# Plotting for each randomly selected image\n",
    "for idx in selected_indices:\n",
    "    image, mask = test_set[idx]\n",
    "\n",
    "    # Convert image and mask to PyTorch tensors\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    mask = mask.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Check the dimensions of the mask\n",
    "    if len(mask.shape) == 3:\n",
    "        _, height, width = mask.shape\n",
    "    elif len(mask.shape) == 4:\n",
    "        _, _, height, width = mask.shape\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected mask dimensions\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(models) + 2, figsize=(18, 4))  # Reduced figure size\n",
    "\n",
    "    ax1, ax2, *model_axes = axes\n",
    "\n",
    "    # Display the original image\n",
    "    ax1.imshow(image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax1.set_title('Image')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Convert ground truth mask to RGB image with class colors\n",
    "    mask_rgb = torch.zeros((3, height, width), dtype=torch.uint8)\n",
    "    for class_idx, color in class_colors.items():\n",
    "        mask_rgb[:, mask.squeeze(0) == class_idx] = torch.tensor(color, dtype=torch.uint8).view(3, 1)\n",
    "\n",
    "    # Display the ground truth mask\n",
    "    ax2.imshow(mask_rgb.permute(1, 2, 0).cpu().numpy())\n",
    "    ax2.set_title('Ground truth')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    for model, ax in zip(models, model_axes):\n",
    "        # Forward pass through the model to get the predicted mask\n",
    "        with torch.no_grad():\n",
    "            output = model(image)  # Forward pass\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze().cpu()  # Convert tensor to CPU and remove batch dimension\n",
    "\n",
    "        # Convert predicted mask to RGB image with class colors\n",
    "        pred_mask_rgb = torch.zeros((3, pred_mask.shape[0], pred_mask.shape[1]), dtype=torch.uint8)\n",
    "        for class_idx, color in class_colors.items():\n",
    "            pred_mask_rgb[:, pred_mask == class_idx] = torch.tensor(color, dtype=torch.uint8).view(3, 1)\n",
    "\n",
    "        # Calculate Intersection over Union (IoU)\n",
    "        iou, iou_per_class = mIoU(output, mask)\n",
    "\n",
    "        # Plot the predicted mask\n",
    "        ax.imshow(pred_mask_rgb.permute(1, 2, 0).cpu().numpy())\n",
    "        ax.set_title(f'Model {models.index(model) + 1}\\nIoU: {iou:.4f}')  # Include IoU in the title\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Adjust layout to make room for the legend\n",
    "    plt.tight_layout(pad=0.1)\n",
    "    plt.subplots_adjust(left=0.02, right=0.98, top=0.95, bottom=0.15, wspace=0.05)\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "l_A6xGBTMLfQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# translucent background\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes):  # loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "            union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "            iou = (intersect + smooth) / (union + smooth)\n",
    "            iou_per_class.append(iou)\n",
    "\n",
    "        mean_iou = np.nanmean(iou_per_class)\n",
    "        return mean_iou, iou_per_class\n",
    "\n",
    "# Define class colors and titles\n",
    "class_colors = {\n",
    "    0: [91, 6, 117],  # Background purple\n",
    "    1: [255, 0, 0],   # Dry down is red\n",
    "    2: [0, 0, 255],   # Nutrient deficiency is blue\n",
    "    3: [0, 153, 0],   # Weed cluster is green\n",
    "}\n",
    "\n",
    "class_titles = {\n",
    "    0: 'Background',\n",
    "    1: 'Dry down',\n",
    "    2: 'Nutrient deficiency',\n",
    "    3: 'Weed cluster',\n",
    "}\n",
    "\n",
    "# Load the models\n",
    "model_paths = [\"/content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth\",\"/content/RJB_514_Unet-Mobilenetv2-transformations.pth\", \"/content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth\", \"/content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth\"]\n",
    "models = []\n",
    "for path in model_paths:\n",
    "    model = torch.load(path)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "# Randomly select indices from the test set\n",
    "selected_indices = list(range(1, 55)) + list(range(150, 200)) + list(range(250, 300)) + list(range(450, 500)) + list(range(545, 600)) + list(range(700, 725))\n",
    "\n",
    "# Plotting for each randomly selected image\n",
    "for idx in selected_indices:\n",
    "    image, mask = test_set[idx]\n",
    "\n",
    "    # Convert image and mask to PyTorch tensors\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    mask = mask.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Check the dimensions of the mask\n",
    "    if len(mask.shape) == 3:\n",
    "        _, height, width = mask.shape\n",
    "    elif len(mask.shape) == 4:\n",
    "        _, _, height, width = mask.shape\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected mask dimensions\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(models) + 2, figsize=(18, 4))  # Reduced figure size\n",
    "\n",
    "    # Set the background of the figure to be translucent\n",
    "    fig.patch.set_alpha(0)\n",
    "    for ax in axes:\n",
    "        ax.patch.set_alpha(0)\n",
    "\n",
    "    ax1, ax2, *model_axes = axes\n",
    "\n",
    "    # Display the original image\n",
    "    ax1.imshow(image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax1.set_title('Image')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Convert ground truth mask to RGB image with class colors\n",
    "    mask_rgb = torch.zeros((3, height, width), dtype=torch.uint8)\n",
    "    for class_idx, color in class_colors.items():\n",
    "        mask_rgb[:, mask.squeeze(0) == class_idx] = torch.tensor(color, dtype=torch.uint8).view(3, 1)\n",
    "\n",
    "    # Display the ground truth mask\n",
    "    ax2.imshow(mask_rgb.permute(1, 2, 0).cpu().numpy())\n",
    "    ax2.set_title('Ground truth')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    for model, ax in zip(models, model_axes):\n",
    "        # Forward pass through the model to get the predicted mask\n",
    "        with torch.no_grad():\n",
    "            output = model(image)  # Forward pass\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze().cpu()  # Convert tensor to CPU and remove batch dimension\n",
    "\n",
    "        # Convert predicted mask to RGB image with class colors\n",
    "        pred_mask_rgb = torch.zeros((3, pred_mask.shape[0], pred_mask.shape[1]), dtype=torch.uint8)\n",
    "        for class_idx, color in class_colors.items():\n",
    "            pred_mask_rgb[:, pred_mask == class_idx] = torch.tensor(color, dtype=torch.uint8).view(3, 1)\n",
    "\n",
    "        # Calculate Intersection over Union (IoU)\n",
    "        iou, iou_per_class = mIoU(output, mask)\n",
    "\n",
    "        # Plot the predicted mask\n",
    "        ax.imshow(pred_mask_rgb.permute(1, 2, 0).cpu().numpy())\n",
    "        ax.set_title(f'Model {models.index(model) + 1}\\nIoU: {iou:.4f}')  # Include IoU in the title\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Adjust layout to make room for the legend\n",
    "    plt.tight_layout(pad=0.1)\n",
    "    plt.subplots_adjust(left=0.02, right=0.98, top=0.95, bottom=0.15, wspace=0.05)\n",
    "\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "TZerdsPGhP6H",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "85ea6bd8-1e00-46fc-b390-74390a299c60"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# translucent background and vertical\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=4):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes):  # loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "            union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "            iou = (intersect + smooth) / (union + smooth)\n",
    "            iou_per_class.append(iou)\n",
    "\n",
    "        mean_iou = np.nanmean(iou_per_class)\n",
    "        return mean_iou, iou_per_class\n",
    "\n",
    "# Define class colors and titles\n",
    "class_colors = {\n",
    "    0: [91, 6, 117],  # Background purple\n",
    "    1: [255, 0, 0],   # Dry down is red\n",
    "    2: [0, 0, 255],   # Nutrient deficiency is blue\n",
    "    3: [0, 153, 0],   # Weed cluster is green\n",
    "}\n",
    "\n",
    "class_titles = {\n",
    "    0: 'Background',\n",
    "    1: 'Dry down',\n",
    "    2: 'Nutrient deficiency',\n",
    "    3: 'Weed cluster',\n",
    "}\n",
    "\n",
    "# Load the models\n",
    "model_paths = [\"/content/RJB_513_Unet-Mobilenetv2-resize-no-transform.pth\",\"/content/RJB_514_Unet-Mobilenetv2-transformations.pth\", \"/content/RJB_514_Unet-Mobilenetv2-transformations_no_transfer_learning.pth\", \"/content/RJB_517_DeepLabV3Plus-Mobilenetv2-transformations_yes_transfer_learning.pth\"]\n",
    "models = []\n",
    "for path in model_paths:\n",
    "    model = torch.load(path)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "# Randomly select indices from the test set\n",
    "selected_indices = list(range(1, 55)) + list(range(150, 200)) + list(range(250, 300)) + list(range(450, 500)) + list(range(545, 600)) + list(range(700, 725))\n",
    "\n",
    "# Plotting for each randomly selected image\n",
    "for idx in selected_indices:\n",
    "    image, mask = test_set[idx]\n",
    "\n",
    "    # Convert image and mask to PyTorch tensors\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    mask = mask.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Check the dimensions of the mask\n",
    "    if len(mask.shape) == 3:\n",
    "        _, height, width = mask.shape\n",
    "    elif len(mask.shape) == 4:\n",
    "        _, _, height, width = mask.shape\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected mask dimensions\")\n",
    "\n",
    "    # Change 1: Update subplot layout to be vertical\n",
    "    fig, axes = plt.subplots(len(models) + 2, 1, figsize=(5, 18))  # Changed figure size and layout to vertical\n",
    "\n",
    "    # Set the background of the figure to be translucent\n",
    "    fig.patch.set_alpha(0)\n",
    "    for ax in axes:\n",
    "        ax.patch.set_alpha(0)\n",
    "\n",
    "    ax1, ax2, *model_axes = axes\n",
    "\n",
    "    # Display the original image\n",
    "    ax1.imshow(image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax1.set_title('Image')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Convert ground truth mask to RGB image with class colors\n",
    "    mask_rgb = torch.zeros((3, height, width), dtype=torch.uint8)\n",
    "    for class_idx, color in class_colors.items():\n",
    "        mask_rgb[:, mask.squeeze(0) == class_idx] = torch.tensor(color, dtype=torch.uint8).view(3, 1)\n",
    "\n",
    "    # Display the ground truth mask\n",
    "    ax2.imshow(mask_rgb.permute(1, 2, 0).cpu().numpy())\n",
    "    ax2.set_title('Ground truth')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    for model, ax in zip(models, model_axes):\n",
    "        # Forward pass through the model to get the predicted mask\n",
    "        with torch.no_grad():\n",
    "            output = model(image)  # Forward pass\n",
    "            pred_mask = torch.argmax(output, dim=1).squeeze().cpu()  # Convert tensor to CPU and remove batch dimension\n",
    "\n",
    "        # Convert predicted mask to RGB image with class colors\n",
    "        pred_mask_rgb = torch.zeros((3, pred_mask.shape[0], pred_mask.shape[1]), dtype=torch.uint8)\n",
    "        for class_idx, color in class_colors.items():\n",
    "            pred_mask_rgb[:, pred_mask == class_idx] = torch.tensor(color, dtype=torch.uint8).view(3, 1)\n",
    "\n",
    "        # Calculate Intersection over Union (IoU)\n",
    "        iou, iou_per_class = mIoU(output, mask)\n",
    "\n",
    "        # Plot the predicted mask\n",
    "        ax.imshow(pred_mask_rgb.permute(1, 2, 0).cpu().numpy())\n",
    "        ax.set_title(f'Model {models.index(model) + 1}\\nIoU: {iou:.4f}')  # Include IoU in the title\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Change 2: Adjust layout to fit the new vertical arrangement\n",
    "    plt.tight_layout(pad=0.1)\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.95, bottom=0.05, hspace=0.5)\n",
    "\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-lLbkMswiy8_",
    "outputId": "81790f57-6103-42cc-a325-5b19f325eb9e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random  # Add this line to import the random module\n",
    "\n",
    "# Randomly select indices from the test set\n",
    "selected_indices = random.sample(range(len(test_set)), 75)\n",
    "\n",
    "# Plot each randomly selected image individually\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    image, _ = test_set[idx]  # Ignore the mask\n",
    "\n",
    "    # Convert image to numpy array and plot\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "\n",
    "    # Set the background of the figure and axes to be completely transparent\n",
    "    plt.gca().set_facecolor('none')\n",
    "    plt.gcf().patch.set_alpha(0)\n",
    "    plt.gca().patch.set_alpha(0)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "TEoHIb0ahRY8"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "535763aaca514cacb8ce2a82edefbb92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a25f7683909c4a7291b46c9289f5d93f",
       "IPY_MODEL_e36e9cf856e2480684b1ed75d3f32685",
       "IPY_MODEL_c3a4cb85e92648c4abcdd33932bfb39b"
      ],
      "layout": "IPY_MODEL_895b8ca72db947f2a051e06d0760bae1"
     }
    },
    "a25f7683909c4a7291b46c9289f5d93f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2809026fc387425fbb7c18c53ed4ff7c",
      "placeholder": "​",
      "style": "IPY_MODEL_ed67466e8958447e8b2abe1b5f440316",
      "value": "100%"
     }
    },
    "e36e9cf856e2480684b1ed75d3f32685": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b28618c93ce14b27b9acdf6a6630c326",
      "max": 47,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_21c8f913d4f24e59adbbebbcc742b909",
      "value": 47
     }
    },
    "c3a4cb85e92648c4abcdd33932bfb39b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_628356cbf9f44216a4d7bf4137a5f7a8",
      "placeholder": "​",
      "style": "IPY_MODEL_e762590e866b4023a480942769689a3d",
      "value": " 47/47 [05:11&lt;00:00,  6.20s/it]"
     }
    },
    "895b8ca72db947f2a051e06d0760bae1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2809026fc387425fbb7c18c53ed4ff7c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed67466e8958447e8b2abe1b5f440316": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b28618c93ce14b27b9acdf6a6630c326": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21c8f913d4f24e59adbbebbcc742b909": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "628356cbf9f44216a4d7bf4137a5f7a8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e762590e866b4023a480942769689a3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
